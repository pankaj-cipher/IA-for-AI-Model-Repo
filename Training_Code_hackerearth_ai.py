# -*- coding: utf-8 -*-
"""Copy of Hackerearth_AI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tSPyRjEbqCBA4q7lvntrY4uAieNShIIz
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
from tensorflow.keras.layers import LSTM, Dense , Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
import tensorflow
import glob
import librosa
from google.colab import drive 
drive.mount('/gdrive', force_remount=True)

!cp /gdrive/My\ Drive/data.zip data.zip
!mkdir data_hacker
!unzip data.zip -d data_hacker

data_train = pd.read_csv('/content/data_hacker/dataset/train.csv')
data_test = pd.read_csv('/content/data_hacker/dataset/test.csv')

emotions = data_train['emotion'].unique()
emotions.sort()
emotions

data_train.info()

data_test.info()

l=[]
for i in data_train.index:
  for j in range(len(emotions)):
    if emotions[j]==data_train['emotion'][i]:
      l.append(j)
      break
train_label = np.array(l)

new_emotions = ['anger', 'disgust', 'fear', 'joy', 'sadness','surprise']

l=[]
for i in data_train.index:
  for j in range(len(new_emotions)):
    if new_emotions[j]==data_train['emotion'][i]:
      l.append(j)
      break
new_train_label = np.array(l)

new_train=[]
import librosa.display
for i in data_train.index:
  y,sr = librosa.load('/content/data_hacker/dataset/TrainAudioFiles/'+data_train['filename'][i])
  new_train.append((y,sr))
  print(i)
new_test=[]
for i in data_test.index:
  y,sr = librosa.load('/content/data_hacker/dataset/TestAudioFiles/'+data_test['filename'][i])
  new_test.append((y,sr))
  print(i)
  #librosa.display.waveplot(y,sr=sr, x_axis='time', color='purple',offset=0.0)
  #break

new_train1=[]
import librosa.display
for i in data_train.index:
  y,sr = librosa.load('/content/data_hacker/dataset/TrainAudioFiles/'+data_train['filename'][i],duration=10)
  new_train1.append((y,sr))
  mfccs = librosa.feature.mfcc(y=y, sr=sr,n_mfcc=40)
  chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)
  #rmse = librosa.feature.rmse(y=y)
  spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)
  spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)
  rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
  zcr = librosa.feature.zero_crossing_rate(y)
  print(mfccs.shape,chroma_stft.shape,spec_cent.shape,spec_bw.shape,rolloff.shape,zcr.shape)
  if i ==2:
    break
new_test1=[]
for i in data_test.index:
  y,sr = librosa.load('/content/data_hacker/dataset/TestAudioFiles/'+data_test['filename'][i],duration=10)
  new_test1.append((y,sr))
  mfccs = librosa.feature.mfcc(y=y ,sr=sr,n_mfcc=40)
  print(mfccs.shape)
  if i ==2:
    break
  #librosa.display.waveplot(y,sr=sr, x_axis='time', color='purple',offset=0.0)
  #break

import pickle
data_save = {'train':new_train,'test':new_test}
with open('hacker_ai.pckl', 'wb') as output_file:
  pickle.dump(data_save, output_file)

!cp hacker_ai.pckl /gdrive/My\ Drive/hacker_ai.pckl

!cp /gdrive/My\ Drive/hacker_ai.pckl hacker_ai.pckl

import pickle
file = open('hacker_ai.pckl', 'rb')
data_save = pickle.load(file, encoding='latin1')  # <<---- change to 'latin1' to 'utf8' if the data does not load
file.close()

len(data_save['test'])

max1 = 0
min1 = 9261
for i,j in data_save['train']:
  if max1 < len(i):
    max1 = len(i)
  if min1 > len(i):
    min1 = len(i)
print(max1)
print(min1)

s=0
for i,j in data_save['train']:
  s = s+len(i)
print(s/len(data_save['train']))

duration = []
for i,j in data_save['train']:
  dur = librosa.get_duration(y=i, sr=j)
  duration.append(dur)
print(min(duration))
print(max(duration))

train_neutral = []
k = 0
k1=0
for i in range(len(train_label)):
  if train_label[i] == 4:
    train_neutral.append(1)
    k += 1
  else:
    train_neutral.append(0)
    k1+=1
print(k,k1)

train_neutral = np.array(train_neutral)

list_mfccs = []
z= 0
input_length = 1*22050
for i,j in data_save['train']:
  y = librosa.util.fix_length(i, input_length)
  mfccs = librosa.feature.mfcc(y=y, sr=j,n_mfcc=10)
  #rms = librosa.feature.rms(y=y)
  new = librosa.feature.delta(mfccs,order = 1)
  new1 = librosa.feature.delta(mfccs , order = 2)
  #chroma_stft = librosa.feature.chroma_stft(y=y, sr=j)
  #oenv = librosa.onset.onset_strength(y=y, sr=j)
  #tempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=j)
  #spec_cent = librosa.feature.spectral_centroid(y=y, sr=j)
  #spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=j)
  #rolloff = librosa.feature.spectral_rolloff(y=y, sr=j)
  #zcr = librosa.feature.zero_crossing_rate(y)
  comb = np.concatenate((mfccs,new,new1),axis = 0)
  #comb = np.concatenate((mfccs,new,new1,chroma_stft),axis = 0)
  feature = comb.reshape(1320,1)
  list_mfccs.append(feature)
#print(mfccs.shape,rms.shape,chroma_stft.shape,chroma_cqt.shape,chroma_cens.shape,spec_cent.shape,spec_cont.shape,spec_flat.shape,spec_bw.shape,rolloff.shape,poly.shape,ton.shape,zcr.shape)
len(list_mfccs)

list_mfccs1 = []
input_length = 1*22050
for i,j in data_save['test']:
  y = librosa.util.fix_length(i, input_length)
  mfccs = librosa.feature.mfcc(y=y, sr=j,n_mfcc=10)
  #rms = librosa.feature.rms(y=y)
  new = librosa.feature.delta(mfccs,order = 1)
  new1 = librosa.feature.delta(mfccs , order = 2)
  #chroma_stft = librosa.feature.chroma_stft(y=y, sr=j)
  #oenv = librosa.onset.onset_strength(y=y, sr=j)
  #tempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=j)
  #spec_cent = librosa.feature.spectral_centroid(y=y, sr=j)
  #spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=j)
  #rolloff = librosa.feature.spectral_rolloff(y=y, sr=j)
  #zcr = librosa.feature.zero_crossing_rate(y)
  comb = np.concatenate((mfccs,new,new1),axis = 0)
  #comb = np.concatenate((mfccs,new,new1,chroma_stft),axis = 0)
  feature = comb.reshape(1320,1)
  list_mfccs1.append(feature)
#print(mfccs.shape,rms.shape,chroma_stft.shape,chroma_cqt.shape,chroma_cens.shape,spec_cent.shape,spec_cont.shape,spec_flat.shape,spec_bw.shape,rolloff.shape,poly.shape,ton.shape,zcr.shape)
len(list_mfccs1)

#list_mfccs = []
for i,j in data_save['train']:
  mfccs = np.mean(librosa.feature.mfcc(y=i, sr=j,n_mfcc=40).T,axis = 0)
  feature = np.array(mfccs).reshape([-1,1])
  list_mfccs.append(feature)
len(list_mfccs)

#new aug :
new_list_aug = []
new_labels_aug = []
input_length = 3*22050
for r in range(len(data_save['train'])):
  perm=[] 
  if train_label[r]== 1 or train_label[r]== 2 or train_label[r]==5:
    i,j = data_save['train'][r]
    div = len(i)//9
    div1 = 0
    #perm = []
    for com in range(9):
      tmp = []
      for com1 in range(div+1):
        if div1 <  len(i):
          tmp.append(i[div1])
          div1+=1
      perm.append(tmp)
      #new_labels_aug.append(train_label[r])  
  elif train_label[r]== 0:
    i,j = data_save['train'][r]
    div = len(i)//5
    div1 = 0
    #perm = []
    for com in range(5):
      tmp = []
      for com1 in range(div+1):
        if div1 <  len(i):
          tmp.append(i[div1])
          div1+=1
      perm.append(tmp)
      #new_labels_aug.append(train_label[r])   
  elif train_label[r]== 6:
    i,j = data_save['train'][r]
    div = len(i)//4
    div1 = 0
    #perm = []
    for com in range(4):
      tmp = []
      for com1 in range(div+1):
        if div1 <  len(i):
          tmp.append(i[div1])
          div1+=1
      perm.append(tmp)
      #new_labels_aug.append(train_label[r]) 
  elif train_label[r]== 3:
    i,j = data_save['train'][r]
    div = len(i)//3
    div1 = 0
    #perm = []
    for com in range(3):
      tmp = []
      for com1 in range(div+1):
        if div1 <  len(i):
          tmp.append(i[div1])
          div1+=1
      perm.append(tmp)
      #new_labels_aug.append(train_label[r]) 
    else:
      i,j = data_save['train'][r]
      perm.append(i)
  for p in perm:
      p = np.array(p)
      y = librosa.util.fix_length(p, input_length)
      mfccs = librosa.feature.mfcc(y=y, sr=j,n_mfcc=13)
      chroma_stft = librosa.feature.chroma_stft(y=y, sr=j)
      spec_cent = librosa.feature.spectral_centroid(y=y, sr=j)
      spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=j)
      rolloff = librosa.feature.spectral_rolloff(y=y, sr=j)
      zcr = librosa.feature.zero_crossing_rate(y)
      comb = np.concatenate((mfccs,chroma_stft,spec_cent,spec_bw,rolloff,zcr),axis = 0)
      feature = comb.reshape(3770,1)
      new_list_aug.append(feature)
      new_labels_aug.append(train_label[r])
#print(mfccs.shape,chroma_stft.shape,spec_cent.shape,spec_bw.shape,rolloff.shape,zcr.shape)
len(new_list_aug)

#explicit aug
new_list_mfccs = []
new_labels = []
input_length = 3*22050
for r in range(len(data_save['train'])):
  if train_label[r]== 1 or train_label[r]== 2 or train_label[r]==5:
    i,j = data_save['train'][r]
    i = i[::-1]
    y = librosa.util.fix_length(i, input_length)
    mfccs = librosa.feature.mfcc(y=y, sr=j,n_mfcc=13)
    chroma_stft = librosa.feature.chroma_stft(y=y, sr=j)
    spec_cent = librosa.feature.spectral_centroid(y=y, sr=j)
    spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=j)
    rolloff = librosa.feature.spectral_rolloff(y=y, sr=j)
    zcr = librosa.feature.zero_crossing_rate(y)
    comb = np.concatenate((mfccs,chroma_stft,spec_cent,spec_bw,rolloff,zcr),axis = 0)
    feature = comb.reshape(3770,1)
    new_list_mfccs.append(feature)
    new_labels.append(train_label[r])
#print(mfccs.shape,chroma_stft.shape,spec_cent.shape,spec_bw.shape,rolloff.shape,zcr.shape)
len(new_list_mfccs)

a = np.array([1,2,3])
a[::-1]

# new_list_aug1  = np.array(new_list_aug)
# new_list_aug1.shape
arr_mfccs = np.array(list_mfccs)
arr_mfccs.shape

#new_labels_aug = np.array(new_labels_aug)
#new_labels_aug.shape
final_label = np.array(train_label)
final_label.shape

list_mfccs1 = []
input_length = 5*22050
for i,j in data_save['test']:
  y = librosa.util.fix_length(i, input_length)
  mfccs = librosa.feature.mfcc(y=y, sr=j,n_mfcc=13)
  feature = mfccs.reshape(2808,1)
  list_mfccs1.append(feature)
len(list_mfccs1)

list_mfccs1 = []
for i,j in data_save['test']:
  mfccs = np.mean(librosa.feature.mfcc(y=i, sr=j,n_mfcc=40).T,axis = 0)
  feature = np.array(mfccs).reshape([-1,1])
  list_mfccs1.append(feature)
len(list_mfccs1)

arr_mfccs1 = np.array(list_mfccs1)
arr_mfccs1.shape

print(np.min(arr_mfccs))
print(np.max(arr_mfccs))
print(np.min(arr_mfccs1))
print(np.max(arr_mfccs1))

def norm_data(data,min, max):
  data = data - min
  data = (data / (max-min))
  return data

arr_mfcss1 = arr_mfccs1.reshape(2492, 1320)
arr_mfcss = arr_mfccs.reshape(5816, 1320)
#new_list_aug1 = new_list_aug1.reshape(5816, 7776)

from sklearn.preprocessing import StandardScaler
s = StandardScaler()
final_train = s.fit_transform(arr_mfcss)
#final_train = s.fit_transform(new_list_aug1)
final_test = s.fit_transform(arr_mfcss1)

final_test = final_test.reshape(2492, 3770,1)
final_train = final_train.reshape(18255, 3770,1)

final_train = norm_data(arr_mfccs,min = -1200, max = 250)
final_test = norm_data(arr_mfccs1,min = -1200, max = 250)

print(final_train.shape)
print(final_test.shape)

counter = 0
indexes = []
for k in range(7):
  ind =[]
  for i in range(len(train_label)):
    if train_label[i] == k:
      ind.append(i)
    if len(ind)==311:
      break
  indexes.append(ind)

train_label = list(train_label)
for i in new_labels:
  train_label.append(i)
train_label = np.array(train_label)

final1=[]
final_label =[]
for i in indexes:
  for j in i:
    final1.append(final_train[j])
    final_label.append(train_label[j])

final_train = np.array(final1)
final_label = np.array(final_label)

print(final_train.shape)
print(final_label.shape)

final_label = to_categorical(train_label)

!cp /gdrive/My\ Drive/hacker_ai_new.pckl hacker_ai_new.pckl

!cp /gdrive/My\ Drive/hacker_ai_new1.pckl hacker_ai_new1.pckl

import pickle
data_save = {'x_train':final_train,'y_train':final_label,'x_test':final_test}
with open('hacker_ai_new.pckl', 'wb') as output_file:
  pickle.dump(data_save, output_file)

!cp hacker_ai_new.pckl /gdrive/My\ Drive/hacker_ai_new.pckl

import pickle
data_save = {'x_train':final_train,'y_train':final_label,'x_test':final_test}
with open('hacker_ai_new1.pckl', 'wb') as output_file:
  pickle.dump(data_save, output_file)

!cp hacker_ai_new1.pckl /gdrive/My\ Drive/hacker_ai_new1.pckl

import pickle
file = open('hacker_ai_new.pckl', 'rb')
data_save = pickle.load(file, encoding='latin1')  # <<---- change to 'latin1' to 'utf8' if the data does not load
file.close()

import pickle
file = open('hacker_ai_new1.pckl', 'rb')
data_save = pickle.load(file, encoding='latin1')  # <<---- change to 'latin1' to 'utf8' if the data does not load
file.close()

#data_save['x_train']:
k1 = [596, 311, 328, 967, 2630, 344, 640]
k = [0]*7
label_list = []
feature_list = []
tmp =0
tmp1 = 0
tmp2 = 0
tmp3 = 0
tmp4 = 0
tmp5 = 0
for i in range(len(data_save['y_train'])):
  if data_save['y_train'][i][4] ==1:
    if tmp ==311:
      continue
    else:
      tmp+=1
      label_list.append(data_save['y_train'][i])
      feature_list.append(data_save['x_train'][i])
  elif data_save['y_train'][i][3] ==1:
    if tmp1 ==311:
      continue
    else:
      tmp1+=1
      label_list.append(data_save['y_train'][i])
      feature_list.append(data_save['x_train'][i])
  elif data_save['y_train'][i][0] ==1:
    if tmp2 ==311:
      continue
    else:
      tmp2+=1
      label_list.append(data_save['y_train'][i])
      feature_list.append(data_save['x_train'][i])
  elif data_save['y_train'][i][6] ==1:
    if tmp3 ==311:
      continue
    else:
      tmp3+=1
      label_list.append(data_save['y_train'][i])
      feature_list.append(data_save['x_train'][i])
  elif data_save['y_train'][i][5] ==1:
    if tmp4 ==311:
      continue
    else:
      tmp4+=1
      label_list.append(data_save['y_train'][i])
      feature_list.append(data_save['x_train'][i])
  elif data_save['y_train'][i][2] ==1:
    if tmp5 ==311:
      continue
    else:
      tmp5+=1
      label_list.append(data_save['y_train'][i])
      feature_list.append(data_save['x_train'][i])
  else:
    label_list.append(data_save['y_train'][i])
    feature_list.append(data_save['x_train'][i])
label_list = np.array(label_list)
feature_list = np.array(feature_list)
print(label_list.shape,feature_list.shape)

k

d = np.array([0,0,0,0,0,0,0])
final_label = np.array(final_label)
for i in final_label:
  d =d+i
print(d)

k = [0]*7
label_list = []
feature_list = []
tmp =0
for i in range(len(final_train)):
  if final_label[i][4] ==1:
    if tmp ==1000:
      continue
    else:
      tmp+=1
      label_list.append(final_label[i])
      feature_list.append(final_train[i])
  else:
    label_list.append(final_label[i])
    feature_list.append(final_train[i])
label_list = np.array(label_list)
feature_list = np.array(feature_list)
print(label_list.shape,feature_list.shape)

from tensorflow.keras.layers import Conv1D , BatchNormalization

model = Sequential()
#model.add(Conv1D(128,5,input_shape = (data_save['x_train'].shape[1],1)))
model.add(Conv1D(128,5,input_shape = (feature_list.shape[1],1)))
#model.add(BatchNormalization())
#model.add(LSTM(64,return_sequences= True))
model.add(LSTM(64))
model.add(Dense(64,activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(7,activation = 'softmax'))
model.summary()

data_save['x_train']=data_save['x_train'].reshape(5816, 3770)
data_save['x_test'] = data_save['x_test'].reshape(2492, 3770)

final_train = final_train.reshape(6799, 3770)
final_test = final_test.reshape(2492, 3770)

final_test = final_test.reshape(2492, 3770)
final_train = final_train.reshape(18255, 3770)

!cp /gdrive/My\ Drive/hacker_ai_baap3.pckl hacker_ai_baap3.pckl
import pickle
file = open('hacker_ai_baap3.pckl', 'rb')
data_save = pickle.load(file, encoding='latin1')  # <<---- change to 'latin1' to 'utf8' if the data does not load
file.close()

from sklearn import svm
SVM = svm.SVC()
SVM.fit(final_train,train_label)
pred = SVM.predict(final_test)
#pred = np.argmax(pred, axis=1)
labels = []
for i in pred:
  labels.append(emotions[i])
data_test['emotion'] = labels
data_test.to_csv('Submission.csv',index=False)

from sklearn import ensemble
RF = ensemble.RandomForestClassifier(n_estimators = 100, criterion='entropy', oob_score= True, n_jobs  = -1)
#RF.fit(data_save['x_train'],data_save['y_train'])
#pred = RF.predict(data_save['x_test'])
#pred = np.argmax(pred, axis=1)
RF.fit(final_train,train_label)
pred = RF.predict(final_test)
labels = []
for i in pred:
  labels.append(emotions[i])
data_test['emotion'] = labels

data_test.to_csv('Submission3.csv',index=False)

#new_train = []
#from sklearn import ensemble
#RF = ensemble.RandomForestClassifier(n_estimators = 1000,random_state=0, criterion='entropy', oob_score= True, n_jobs  = -1)
from sklearn import svm
SVM = svm.SVC()
SVM.fit(final_train,train_neutral)
pred = SVM.predict(final_test)
#RF.fit(data_save['x_train'],data_save['y_train'])
#pred = RF.predict(data_save['x_test'])
#pred = np.argmax(pred, axis=1)
SVM.fit(final_train,train_neutral)
pred = SVM.predict(final_test)
labels = []
for i in pred:
  if i == 1:
    labels.append(emotions[4])
  else:
    labels.append(0)
#data_test['emotion'] = labels
#data_test.to_csv('Submission3.csv',index=False)

new_train = []
new_label = []
new_test = []
for i in range(len(final_train)):
  if final_label[i] < 4:
    new_label.append(final_label[i])
    new_train.append(final_train[i])
  elif final_label[i] > 4:
    new_label.append(final_label[i]-1)
    new_train.append(final_train[i])
for i in range(len(final_test)):
  if labels[i] == 0:
    new_test.append(final_test[i])
new_train = np.array(new_train)
new_label = np.array(new_label)
new_test = np.array(new_test)
print(new_train.shape,new_label.shape,new_test.shape)

from sklearn import ensemble
RF = ensemble.RandomForestClassifier(n_estimators = 600, random_state= 0,criterion='entropy', oob_score= True, n_jobs  = -1)
#RF.fit(data_save['x_train'],data_save['y_train'])
#pred = RF.predict(data_save['x_test'])
#pred = np.argmax(pred, axis=1)
RF.fit(new_train,new_label)
pred = RF.predict(new_test)
new_labels = []
for i in pred:
  new_labels.append(new_emotions[i])

k=0
for i in range(len(labels)):
  if labels[i] == 0:
    labels[i] = new_labels[k]
    k+=1

data_test['emotion'] = labels
data_test.to_csv('Submission3.csv',index=False)

from sklearn import ensemble
for j in range(0,50):
  RF = ensemble.RandomForestClassifier(n_estimators = 100 , criterion='entropy',random_state =j  , oob_score= True, n_jobs  = -1)
  #RF.fit(data_save['x_train'],data_save['y_train'])
  #pred = RF.predict(data_save['x_test'])
  #pred = np.argmax(pred, axis=1)
  RF.fit(final_train,train_label)
  pred = RF.predict(final_test)
  labels = []
  for i in pred:
    labels.append(emotions[i])
  data_test['emotion'] = labels
  s = 'Submission'+str(j)+'.csv'
  print(s)
  data_test.to_csv(s,index=False)

filename = 'RFC.pckl'
pickle.dump(RF, open(filename, 'wb'))

# load the model from disk.
loaded_model = pickle.load(open(filename, 'rb'))

from xgboost import XGBClassifier
model = XGBClassifier(n_estimators = 660,random_state = 0)
model.fit(final_train,train_label)
pred = model.predict(final_test)
labels = []
for i in pred:
  labels.append(emotions[i])
data_test['emotion'] = labels
data_test.to_csv('Submission.csv',index=False)

import pickle
data_save = {'x_train':final_train,'y_train':train_label,'x_test':final_test}
with open('hacker_ai_baap.pckl', 'wb') as output_file:
  pickle.dump(data_save, output_file)

!cp hacker_ai_baap.pckl /gdrive/My\ Drive/hacker_ai_baap.pckl

import pickle
data_save = {'x_train':final_train,'y_train':train_label,'x_test':final_test}
with open('hacker_ai_baap1.pckl', 'wb') as output_file:
  pickle.dump(data_save, output_file)

!cp hacker_ai_baap1.pckl /gdrive/My\ Drive/hacker_ai_baap1.pckl

import pickle
data_save = {'x_train':final_train,'y_train':train_label,'x_test':final_test}
with open('hacker_ai_baap4.pckl', 'wb') as output_file:
  pickle.dump(data_save, output_file)

!cp hacker_ai_baap4.pckl /gdrive/My\ Drive/hacker_ai_baap4.pckl

model = Sequential()
model.add(Dense(256,activation='relu',input_shape = (final_train.shape[1],)))
model.add(Dropout(0.2))
model.add(Dense(128,activation = 'relu'))
model.add(Dropout(0.2))
model.add(Dense(64,activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(64,activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(7,activation = 'softmax'))
model.summary()

#lr_schedule = tensorflow.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10**(epoch / 40))
filepath = 'my_best_model{epoch:04d}.hdf5'
checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(filepath=filepath, 
                             monitor='val_accuracy',
                             verbose=1, 
                             save_best_only=False,
                             mode='max')
optimizer = tensorflow.keras.optimizers.Adam(lr=1e-3)
model.compile(optimizer = optimizer,loss = 'categorical_crossentropy',metrics=['accuracy'])



#history = model.fit(data_save['x_train'],data_save['y_train'],epochs = 100,validation_split=0.3,batch_size = 32,shuffle = True ,callbacks = [checkpoint])
history = model.fit(final_train,final_label,epochs = 10,batch_size = 32,shuffle = True ,callbacks = [checkpoint])

plt.semilogx(history.history["lr"], history.history["loss"])
plt.axis([1e-3, 1e-0, 0, 30])

pred = model.predict(data_save['x_test'])

best_model1 = tensorflow.keras.models.load_model('/content/my_best_model0001.hdf5')
best_model2 = tensorflow.keras.models.load_model('/content/my_best_model0002.hdf5')
best_model3 = tensorflow.keras.models.load_model('/content/my_best_model0003.hdf5')
best_model4 = tensorflow.keras.models.load_model('/content/my_best_model0004.hdf5')
best_model5 = tensorflow.keras.models.load_model('/content/my_best_model0005.hdf5')
best_model6 = tensorflow.keras.models.load_model('/content/my_best_model0006.hdf5')
best_model7 = tensorflow.keras.models.load_model('/content/my_best_model0007.hdf5')
best_model8 = tensorflow.keras.models.load_model('/content/my_best_model0008.hdf5')
best_model9 = tensorflow.keras.models.load_model('/content/my_best_model0009.hdf5')
best_model10 = tensorflow.keras.models.load_model('/content/my_best_model0010.hdf5')

data_save['x_test'] = data_save['x_test'].reshape(2492, 3770)

best_pred1 = best_model1.predict(final_test)
best_pred2 = best_model2.predict(final_test)
best_pred3 = best_model3.predict(final_test)
best_pred4 = best_model4.predict(final_test)
best_pred5 = best_model5.predict(final_test)
best_pred6 = best_model6.predict(final_test)
best_pred7 = best_model7.predict(final_test)
best_pred8 = best_model8.predict(final_test)
best_pred9 = best_model9.predict(final_test)
best_pred10 = best_model10.predict(final_test)

#best_pred1 = np.argmax(best_pred1, axis =1)
best_pred2 = np.argmax(best_pred2, axis =1)
best_pred3 = np.argmax(best_pred3, axis =1)
best_pred4 = np.argmax(best_pred4, axis =1)
best_pred5 = np.argmax(best_pred5, axis =1)
best_pred6 = np.argmax(best_pred6, axis =1)
best_pred7 = np.argmax(best_pred7, axis =1)
best_pred8 = np.argmax(best_pred8, axis =1)
best_pred9 = np.argmax(best_pred9, axis =1)
best_pred10 = np.argmax(best_pred10, axis =1)

pred = np.argmax(pred, axis=1)

pred.shape

emotions

labels1 = []
labels2 = []
labels3 = []
labels4 = []
labels5 = []
labels6 = []
labels7 = []
labels8 = []
labels9 = []
labels10 = []
for i in best_pred1:
  labels1.append(emotions[i])
for i in best_pred2:
  labels2.append(emotions[i])
for i in best_pred3:
  labels3.append(emotions[i])
for i in best_pred4:
  labels4.append(emotions[i])
for i in best_pred5:
  labels5.append(emotions[i])
for i in best_pred6:
  labels6.append(emotions[i])
for i in best_pred7:
  labels7.append(emotions[i])
for i in best_pred8:
  labels8.append(emotions[i])
for i in best_pred9:
  labels9.append(emotions[i])
for i in best_pred10:
  labels10.append(emotions[i])

s = set()
for i in labels:
  s.add(i)

s

data_test['emotion'] = labels10

data_test.head()

data_test.to_csv('Submission10.csv')

dic = {}
for i in data_train.index:
  dic[data_train['emotion'][i]] = dic.get(data_train['emotion'][i],0)+1
print(dic)

"""CNN Network for spectogram"""

import matplotlib.pyplot as plt
import librosa.display
import pylab
from tensorflow.keras.layers import Conv2D, MaxPool2D , Dense,Flatten,Dropout
from tensorflow.keras import Sequential
import pickle

spectograms = []
hop_length = 512 # number of samples per time-step in spectrogram
n_mels = 128 # number of bins in spectrogram. Height of image
input_length = 10*22050
for i,j in data_save['train']:
  y = librosa.util.fix_length(i, input_length)
  y = librosa.feature.melspectrogram(y=y,sr=j,n_mels=n_mels,
                                            n_fft=hop_length*2, hop_length=hop_length)
  #f = plt.figure(figsize = (10,4))
  #librosa.display.specshow(librosa.power_to_db(y,ref=np.max),y_axis = 'mel',x_axis ='time')
  #f.savefig('fig.png')
  #pylab.savefig('fig.jpg',bbox_inches = 'tight')
  k = librosa.power_to_db(y,ref=np.max)
  #print(y.shape)
  #plt.show()
  #plt.imshow(k)
  spectograms.append(k)

spectograms_test = []
hop_length = 512 # number of samples per time-step in spectrogram
n_mels = 128 # number of bins in spectrogram. Height of image
input_length = 10*22050
for i,j in data_save['test']:
  y = librosa.util.fix_length(i, input_length)
  y = librosa.feature.melspectrogram(y=y,sr=j,n_mels=n_mels,
                                            n_fft=hop_length*2, hop_length=hop_length)
  #f = plt.figure(figsize = (10,4))
  #librosa.display.specshow(librosa.power_to_db(y,ref=np.max),y_axis = 'mel',x_axis ='time')
  #f.savefig('fig.png')
  #pylab.savefig('fig.jpg',bbox_inches = 'tight')
  k = librosa.power_to_db(y,ref=np.max)
  #print(y.shape)
  #plt.show()
  #plt.imshow(k)
  spectograms_test.append(k)

for i in spectograms:
  print(i.shape)
  break

model = Sequential()
model.add(Conv2D(16,kernel_size =(3,3),input_shape = (288,432,3)))
model.add(MaxPool2D())
model.add(Conv2D(32,kernel_size = (3,3)))
model.add(MaxPool2D())
model.add(Conv2D(64,kernel_size = (3,3)))
model.add(MaxPool2D())
model.add(Flatten())
model.add(Dense(256,activation = 'relu'))
model.add(Dropout(0.4))
model.add(Dense(7,activation = 'relu'))

model.summary()

filepath = 'my_best_model.hdf5'
checkpoint = tensorflow.keras.callbacks.ModelCheckpoint(filepath=filepath, 
                             monitor='val_accuracy',
                             verbose=1, 
                             save_best_only=True,
                             mode='max')
optimizer = tensorflow.keras.optimizers.Adam(lr=1e-3)
model.compile(optimizer = optimizer,loss = 'categorical_crossentropy',metrics=['accuracy'])

spectograms = np.array(spectograms)
spectograms_test = np.array(spectograms_test)
spectograms = spectograms.reshape(5816, 128, 431,1)
spectograms_test = spectograms_test.reshape(2492, 128, 431,1)

print(spectograms.shape,spectograms_test.shape,final_label.shape)

spec = {}
spec['x_train'] = spectograms
spec['y_train'] = final_label
spec['test'] = spectograms_test
with open('hacker_ai_spec.pckl', 'wb') as output_file:
  pickle.dump(spec, output_file)

!cp hacker_ai_spec.pckl /gdrive/My\ Drive/hacker_ai_spec.pckl

!cp /gdrive/My\ Drive/hacker_ai_spec.pckl hacker_ai_spec.pckl

import pickle
file = open('hacker_ai_spec.pckl', 'rb')
spec = pickle.load(file, encoding='latin1')  # <<---- change to 'latin1' to 'utf8' if the data does not load
file.close()

spec['x_train'].shape

model.fit(spec['x_train'],spec['y_train'],epochs = 300,validation_split=0.3, callbacks=[checkpoint])

#data_save['x_train']:
k1 = [596, 311, 328, 967, 2630, 344, 640]
k = [0]*7
label_list = []
feature_list = []
tmp =0
tmp1 = 0
tmp2 = 0
tmp3 = 0
tmp4 = 0
tmp5 = 0
for i in range(len(spec['y_train'])):
  # if spec['y_train'][i][4] ==1:
  #   if tmp ==311:
  #     continue
  #   else:
  #     tmp+=1
  #     label_list.append(spec['y_train'][i])
  #     feature_list.append(spec['x_train'][i])
  # elif spec['y_train'][i][3] ==1:
  #   if tmp1 ==311:
  #     continue
  #   else:
  #     tmp1+=1
  #     label_list.append(spec['y_train'][i])
  #     feature_list.append(spec['x_train'][i])
  # elif spec['y_train'][i][0] ==1:
  #   if tmp2 ==311:
  #     continue
  #   else:
  #     tmp2+=1
  #     label_list.append(spec['y_train'][i])
  #     feature_list.append(spec['x_train'][i])
  if spec['y_train'][i][1] ==1:
    if tmp3 ==311:
      continue
    else:
      # tmp3+=1
      # label_list.append(spec['y_train'][i])
      # feature_list.append(spec['x_train'][i])
      continue
  elif spec['y_train'][i][5] ==1:
    if tmp4 ==311:
      continue
    else:
      # tmp4+=1
      # label_list.append(spec['y_train'][i])
      # feature_list.append(spec['x_train'][i])
      continue
  elif spec['y_train'][i][2] ==1:
    if tmp5 ==311:
      continue
    else:
      #tmp5+=1
      #label_list.append(spec['y_train'][i])
      #feature_list.append(spec['x_train'][i])
      continue
  else:
    label_list.append(spec['y_train'][i])
    feature_list.append(spec['x_train'][i])
label_list = np.array(label_list)
feature_list = np.array(feature_list)
print(label_list.shape,feature_list.shape)

label_list1 = []
for i in label_list:
  label_list1.append(np.array([i[0],i[3],i[4],i[6]]))

label_list1= np.array(label_list1)